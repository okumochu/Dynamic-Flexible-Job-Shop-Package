_wandb:
    value:
        cli_version: 0.21.1
        e:
            bqbj5856r46ubgmw0mbwc33u8mmv0koh:
                codePath: exp_hrl.py
                codePathLocal: exp_hrl.py
                cpu_count: 32
                cpu_count_logical: 64
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "588412764160"
                        used: "448977014784"
                email: okumochu55@gmail.com
                executable: /home/r13725046okumo/miniconda3/bin/python
                git:
                    commit: cdaf2ed73a0ff8d471caa5497e57e1eb63e7028d
                    remote: git@github.com:okumochu/Dynamic-Flexible-Job-Shop-Package.git
                gpu: Quadro RTX 6000
                gpu_count: 2
                gpu_nvidia:
                    - architecture: Turing
                      cudaCores: 4608
                      memoryTotal: "25769803776"
                      name: Quadro RTX 6000
                      uuid: GPU-56f43da4-1deb-c532-3a59-75303aabf3d2
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-41575f01-33f8-a380-7e12-d3927e3fc83c
                host: ubuntu-ProLiant-DL380-Gen10
                memory:
                    total: "134792536064"
                os: Linux-6.11.0-26-generic-x86_64-with-glibc2.39
                program: /home/r13725046okumo/project/Dynamic-Flexible-Job-Shop-Package/exp_hrl.py
                python: CPython 3.13.5
                root: result/exp_hrl/training_process
                startedAt: "2025-09-11T04:40:54.496896Z"
                writerId: bqbj5856r46ubgmw0mbwc33u8mmv0koh
        m: []
        python_version: 3.13.5
        t:
            "1":
                - 1
                - 105
            "2":
                - 1
                - 105
            "3":
                - 2
                - 16
            "4": 3.13.5
            "5": 0.21.1
            "12": 0.21.1
            "13": linux-x86_64
entropy_coef:
    value: 0.01
episodes_per_epoch:
    value: 10
epochs:
    value: 4000
gamma_manager:
    value: 0.9999
gamma_worker:
    value: 0.999
goal_dim:
    value: 16
goal_duration:
    value: 4
goal_duration_ratio:
    value: 12
intrinsic_reward_scale:
    value: 0.5
latent_dim:
    value: 128
manager_lr:
    value: 0.0002
train_per_episode:
    value: 2
worker_lr:
    value: 0.0003
